{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuBg6gsyggrx"
      },
      "source": [
        "1. What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "\n",
        "\n",
        "ans:-\n",
        "\n",
        "### What is a Convolutional Neural Network (CNN)?\n",
        "\n",
        "A Convolutional Neural Network (CNN) is a type of deep learning model specially designed to process image data. It automatically learns spatial features like edges, textures, shapes, and patterns from images using convolution operations.\n",
        "\n",
        "\n",
        "###Difference from Fully Connected Neural Networks (FNNs):\n",
        "\n",
        "CNNs differ from traditional FNNs mainly in architecture and performance. In FNNs, every neuron is connected to all inputs, which causes a huge number of parameters when dealing with images.\n",
        "\n",
        "This makes FNNs slow and prone to overfitting. CNNs, on the other hand, use local connections, shared weights, and reduced parameters, making them much more efficient for image data. As a result, CNNs can extract spatial features and patterns significantly better than FNNs, leading to higher accuracy, faster training, and improved generalisation on image classification, detection, and recognition tasks.\n",
        "\n",
        "---\n",
        "\n",
        "2. Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc31c3c3"
      },
      "source": [
        "### Architecture of LeNet-5\n",
        "\n",
        "LeNet-5 is one of the earliest convolutional neural networks, designed by Yann LeCun et al. in 1998 for handwritten digit recognition. Its architecture consists of seven layers, not counting the input, with trainable parameters.\n",
        "\n",
        "Here's a breakdown of its typical architecture:\n",
        "\n",
        "1.  **Input Layer**: Accepts 32x32 grayscale images, which are larger than the 28x28 digits to allow for variations in positioning.\n",
        "2.  **C1 (Convolutional Layer)**: Applies 6 learnable 5x5 filters with a stride of 1. This results in 6 feature maps of size 28x28. Each neuron in C1 is connected to a 5x5 neighborhood in the input.\n",
        "3.  **S2 (Subsampling/Pooling Layer)**: Applies 6 2x2 average pooling filters with a stride of 2. Each unit sums the inputs from a 2x2 neighborhood in the corresponding C1 feature map and multiplies by a learnable coefficient, then adds a learnable bias and passes through a sigmoid function. This reduces the feature maps to 14x14.\n",
        "4.  **C3 (Convolutional Layer)**: Applies 16 learnable 5x5 filters. This layer connects to subsets of the S2 feature maps rather than all of them, which helps to break symmetry and keep the number of connections manageable. This results in 16 feature maps of size 10x10.\n",
        "5.  **S4 (Subsampling/Pooling Layer)**: Similar to S2, it applies 16 2x2 average pooling filters with a stride of 2, reducing the feature maps to 5x5.\n",
        "6.  **C5 (Convolutional Layer)**: A fully connected convolutional layer with 120 learnable 5x5 filters. Since the input to C5 is 16 feature maps of size 5x5, applying 5x5 filters effectively makes it a fully connected layer to the S4 output, resulting in 120 feature maps of size 1x1.\n",
        "7.  **F6 (Fully Connected Layer)**: Consists of 84 neurons, fully connected to C5. These 84 units represent the features extracted from the input image.\n",
        "8.  **Output Layer**: A fully connected layer with 10 neurons, one for each digit (0-9). It uses a Euclidean Radial Basis Function (RBF) network, where each output unit computes the Euclidean distance between the input vector and its weight vector. The class corresponding to the unit with the smallest distance is chosen.\n",
        "\n",
        "### How LeNet-5 Laid the Foundation for Modern Deep Learning Models\n",
        "\n",
        "LeNet-5 was groundbreaking for several reasons and established key principles that are still fundamental to modern CNNs:\n",
        "\n",
        "*   **Convolutional Layers**: It introduced the concept of using convolutional layers to automatically extract hierarchical features from raw pixel data, eliminating the need for manual feature engineering. This is a cornerstone of all modern CNNs.\n",
        "*   **Subsampling/Pooling Layers**: The use of pooling layers to progressively reduce the spatial dimensions of the feature maps, thus reducing the number of parameters and making the network more robust to small translations and distortions, is a standard practice today.\n",
        "*   **Shared Weights**: The concept of shared weights in convolutional layers significantly reduced the number of parameters, making the network more efficient to train and less prone to overfitting.\n",
        "*   **End-to-End Learning**: LeNet-5 demonstrated an end-to-end learning system, where the network learns directly from raw input images to output classifications, a paradigm that is central to deep learning.\n",
        "*   **Backpropagation for Training**: It effectively used the backpropagation algorithm to train the entire network, including the convolutional and pooling layers, which was crucial for its success.\n",
        "\n",
        "While activation functions and pooling methods have evolved (e.g., ReLU instead of sigmoid, max pooling instead of average pooling), the core architectural components and principles established by LeNet-5 remain the backbone of state-of-the-art deep learning models in computer vision.\n",
        "\n",
        "### Original Research Paper Reference\n",
        "\n",
        "*   **LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*, *86*(11), 2278-2324.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2HP9xM0jky1"
      },
      "source": [
        "3. Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "\n",
        "ans\n",
        "\n",
        "**Comparison of AlexNet and VGGNet**\n",
        "\n",
        "**1. Overview**\n",
        "\n",
        "| Aspect | AlexNet | VGGNet |\n",
        "|--------|---------|--------|\n",
        "| Introduced | 2012 (Krizhevsky et al.) | 2014 (Simonyan & Zisserman) |\n",
        "| Input size | 227×227×3 | 224×224×3 |\n",
        "| Depth | 8 layers (5 conv + 3 FC) | 16–19 layers (13–16 conv + 3 FC) |\n",
        "| Parameters | ~60 million | ~138 million (VGG-16) |\n",
        "\n",
        "**2. Design Principles**\n",
        "\n",
        "- **AlexNet:** Introduced ReLU activations, overlapping max-pooling, dropout in FC layers, and GPU training. Relied on larger convolutional filters (11×11, 5×5) and data augmentation.  \n",
        "- **VGGNet:** Focused on depth and simplicity using only 3×3 convolutional filters, repeated conv + pooling blocks, and uniform architecture to improve feature extraction.\n",
        "\n",
        "**3. Number of Parameters**\n",
        "\n",
        "- AlexNet: ~60M (mostly in FC layers)  \n",
        "- VGGNet (VGG-16): ~138M (FC layers dominate, more depth → more parameters)\n",
        "\n",
        "**4. Performance (ImageNet Top-5 Error)**\n",
        "\n",
        "| Model | Top-5 Error |\n",
        "|-------|------------|\n",
        "| AlexNet | 15.3% |\n",
        "| VGG-16 | 7.3% |\n",
        "\n",
        "**5. Key Innovations**\n",
        "\n",
        "- **AlexNet:** First deep CNN to achieve breakthrough ImageNet performance; ReLU, dropout, data augmentation, GPU training.  \n",
        "- **VGGNet:** Deeper architecture with small filters; modular and repeatable design; foundation for transfer learning.\n",
        "\n",
        "**6. Limitations**\n",
        "\n",
        "- **AlexNet:** Shallow by modern standards, large kernels increase computation, prone to overfitting due to FC layers.  \n",
        "- **VGGNet:** Very memory-intensive, slow to train, high parameter count, not computationally efficient compared to newer architectures (e.g., ResNet).\n",
        "\n",
        "**7. Summary**\n",
        "\n",
        "| Feature | AlexNet | VGGNet |\n",
        "|---------|---------|--------|\n",
        "| Depth | 8 layers | 16–19 layers |\n",
        "| Filter size | Large (11×11, 5×5) | Small (3×3) |\n",
        "| Parameters | Moderate (~60M) | High (~138M) |\n",
        "| Innovations | ReLU, dropout, GPU training, data augmentation | Depth, small filters, uniform modular design |\n",
        "| Performance | Good for its time | Higher accuracy, better feature extraction |\n",
        "| Limitations | Fewer layers, less expressive | Heavy, slow, high memory usage |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDnCL0E_kN5p"
      },
      "source": [
        " 4. What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "\n",
        "ans\n",
        "\n",
        "# Transfer Learning in Image Classification\n",
        "\n",
        "Transfer learning is a machine learning technique where a model developed for one task\n",
        "is reused as the starting point for a model on a different, but related, task. In the\n",
        "context of image classification, this typically involves using a pre-trained\n",
        "convolutional neural network (CNN) such as VGGNet, ResNet, or Inception, which has\n",
        "already learned rich feature representations from a large dataset like ImageNet, and\n",
        "fine-tuning it for a new classification task with a smaller dataset.\n",
        "\n",
        "How Transfer Learning Helps:\n",
        "\n",
        "1. Reduces Computational Costs:\n",
        "   - Training deep CNNs from scratch requires massive datasets and significant\n",
        "     computational resources (GPUs, long training times).\n",
        "   - With transfer learning, the model already has pre-learned feature extractors\n",
        "     (like edges, textures, shapes), so we only need to fine-tune the last few layers\n",
        "     for our specific dataset, drastically reducing training time and resource usage.\n",
        "\n",
        "2. Improves Performance with Limited Data:\n",
        "   - Deep learning models often overfit when trained on small datasets.\n",
        "   - Transfer learning leverages knowledge from large-scale datasets, allowing the model\n",
        "     to generalize better on new tasks even with limited labeled data.\n",
        "\n",
        "Example Workflow:\n",
        "- Take a pre-trained CNN (e.g., ResNet50).\n",
        "- Replace its final classification layer with a new layer matching the number of classes\n",
        "  in your dataset.\n",
        "- Fine-tune the network on your smaller dataset.\n",
        "\n",
        "This approach achieves high accuracy quickly, making transfer learning a widely used\n",
        "technique in practical image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSZREoezky93"
      },
      "source": [
        "5.  Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "ans\n",
        "\n",
        "# Role of Residual Connections in ResNet\n",
        "eResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "\n",
        "ans\n",
        "\n",
        "# Role of Residual Connections in ResNet\n",
        "\n",
        "Residual connections are a key feature of the ResNet (Residual Network) architecture.\n",
        "In a residual block, the input to a set of layers is added directly to the output of\n",
        "those layers via a shortcut or skip connection. Mathematically, a residual block\n",
        "computes:\n",
        "\n",
        "    Output = F(Input) + Input\n",
        "\n",
        "where F(Input) represents the transformation learned by the block (e.g., convolution,\n",
        "batch normalization, activation).\n",
        "\n",
        "Role and Benefits:\n",
        "\n",
        "1. **Addresses the Vanishing Gradient Problem:**\n",
        "   - In very deep networks, gradients can become extremely small during backpropagation,\n",
        "     slowing or preventing the network from learning effectively.  \n",
        "   - Residual connections provide a direct path for the gradient to flow back to earlier\n",
        "     layers, ensuring that learning signals remain strong even in very deep networks.\n",
        "\n",
        "2. **Enables Training of Very Deep CNNs:**\n",
        "   - Traditional deep networks often degrade in performance as depth increases due to\n",
        "     vanishing gradients or difficulty in optimizing.  \n",
        "   - With residual connections, ResNet can successfully train hundreds of layers without\n",
        "     performance degradation.\n",
        "\n",
        "3. **Facilitates Feature Reuse:**\n",
        "   - The network can choose to propagate the input unchanged if learning a residual is\n",
        "     unnecessary, making it easier to learn identity mappings and improving convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-rjMwklk_mg"
      },
      "source": [
        "6.  Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwborKGpgf80",
        "outputId": "22edcf40-b83e-4a1b-d527-3aaa39c8ec04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "422/422 - 30s - 71ms/step - accuracy: 0.8985 - loss: 0.3560 - val_accuracy: 0.9582 - val_loss: 0.1484\n",
            "Epoch 2/10\n",
            "422/422 - 27s - 65ms/step - accuracy: 0.9586 - loss: 0.1372 - val_accuracy: 0.9720 - val_loss: 0.0947\n",
            "Epoch 3/10\n",
            "422/422 - 28s - 66ms/step - accuracy: 0.9723 - loss: 0.0915 - val_accuracy: 0.9792 - val_loss: 0.0724\n",
            "Epoch 4/10\n",
            "422/422 - 45s - 108ms/step - accuracy: 0.9799 - loss: 0.0664 - val_accuracy: 0.9822 - val_loss: 0.0618\n",
            "Epoch 5/10\n",
            "422/422 - 41s - 97ms/step - accuracy: 0.9840 - loss: 0.0515 - val_accuracy: 0.9827 - val_loss: 0.0566\n",
            "Epoch 6/10\n",
            "422/422 - 38s - 89ms/step - accuracy: 0.9862 - loss: 0.0428 - val_accuracy: 0.9840 - val_loss: 0.0529\n",
            "Epoch 7/10\n",
            "422/422 - 39s - 93ms/step - accuracy: 0.9903 - loss: 0.0330 - val_accuracy: 0.9868 - val_loss: 0.0473\n",
            "Epoch 8/10\n",
            "422/422 - 36s - 84ms/step - accuracy: 0.9913 - loss: 0.0285 - val_accuracy: 0.9870 - val_loss: 0.0477\n",
            "Epoch 9/10\n",
            "422/422 - 33s - 79ms/step - accuracy: 0.9926 - loss: 0.0233 - val_accuracy: 0.9845 - val_loss: 0.0526\n",
            "Epoch 10/10\n",
            "422/422 - 27s - 65ms/step - accuracy: 0.9940 - loss: 0.0197 - val_accuracy: 0.9862 - val_loss: 0.0521\n",
            "Test Accuracy: 98.47%\n",
            "Training Time: 344.90 seconds\n"
          ]
        }
      ],
      "source": [
        "# LeNet-5 Implementation on MNIST using TensorFlow/Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# 1. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0,1] and reshape for CNN input\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Define the LeNet-5 architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(6, kernel_size=(5,5), activation='tanh', padding='same', input_shape=(28,28,1)),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2)), # Added pool_size\n",
        "    layers.Conv2D(16, kernel_size=(5,5), activation='tanh'),\n",
        "    layers.AveragePooling2D(pool_size=(2, 2)), # Added pool_size\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(120, activation='tanh'),\n",
        "    layers.Dense(84, activation='tanh'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the model and record training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.1, verbose=2)\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# 5. Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647_7y_QlJdQ"
      },
      "source": [
        "7. Use a pre-trained VGG16 model (via transfer learning) on a small custom\n",
        "dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n",
        "Include your code and result discussion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuwfBzFulFdt",
        "outputId": "b46226c1-5ee7-4ef6-8419-0b546c919a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "\u001b[1m228813984/228813984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Total images: 3670\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n",
            "Number of classes: 5, Class names: ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m 8/92\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26:17\u001b[0m 19s/step - accuracy: 0.2342 - loss: 17.7547"
          ]
        }
      ],
      "source": [
        "# Transfer Learning with VGG16 on a Custom Dataset\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Start: Dataset Setup --- #\n",
        "# Download and extract the 'flower_photos' dataset\n",
        "_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "zip_file = tf.keras.utils.get_file(origin=_URL, fname=\"flower_photos.tgz\", extract=True)\n",
        "# Corrected: zip_file now holds the path to the directory where content was extracted.\n",
        "# The actual image directory is a subfolder named 'flower_photos' within it.\n",
        "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')\n",
        "\n",
        "# The get_file function typically returns the path to the *extracted folder* if `extract=True`\n",
        "# and the archive contains a top-level directory. Let's inspect the `zip_file` to be sure.\n",
        "# After extraction, `zip_file` will be something like `/root/.keras/datasets/flower_photos_extracted`\n",
        "# and the actual images are in `/root/.keras/datasets/flower_photos_extracted/flower_photos`.\n",
        "# So, `base_dir` should be constructed from `zip_file` (the folder where it was extracted).\n",
        "base_dir = zip_file # zip_file is the directory where `flower_photos.tgz` contents were extracted\n",
        "\n",
        "image_count = len(list(tf.io.gfile.glob(str(os.path.join(base_dir, 'flower_photos') + '/*/*.jpg'))))\n",
        "print(f\"Total images: {image_count}\")\n",
        "\n",
        "# Define the paths based on the downloaded dataset\n",
        "train_dir = os.path.join(base_dir, 'flower_photos')\n",
        "val_dir = os.path.join(base_dir, 'flower_photos')\n",
        "\n",
        "# --- End: Dataset Setup --- #\n",
        "\n",
        "img_size = (224, 224)  # VGG16 input size\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    batch_size=32,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=img_size,\n",
        "    batch_size=32,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_ds.class_names)\n",
        "print(f\"Number of classes: {num_classes}, Class names: {train_ds.class_names}\")\n",
        "\n",
        "# 2. Load the pre-trained VGG16 model without the top classification layers\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Add custom top layers\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Train the model and record training time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "# 6. Fine-tuning: unfreeze some top layers of VGG16\n",
        "print(\"\\nStarting fine-tuning of the base model...\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze the first few layers, unfreeze the last convolutional block\n",
        "# VGG16 has 5 convolutional blocks. We'll unfreeze the last block (block5_conv1 to block5_pool)\n",
        "for layer in base_model.layers:\n",
        "    if not layer.name.startswith('block5'): # Freeze layers before 'block5'\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True # Unfreeze block5 layers\n",
        "\n",
        "# Recompile with a lower learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Continue training for fine-tuning\n",
        "history_finetune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5 # Train for a few more epochs for fine-tuning\n",
        ")\n",
        "\n",
        "# 7. Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(val_ds)\n",
        "print(f\"Validation Accuracy after fine-tuning: {val_accuracy*100:.2f}%\")\n",
        "\n",
        "# 8. Result discussion (as requested in the original prompt 7)\n",
        "print(\"\\n--- Result Discussion ---\")\n",
        "print(\"The model was first trained with the VGG16 base frozen, allowing the new classification head to learn the specific features for the flower dataset. After this initial training, a portion of the VGG16 base (the last convolutional block) was unfrozen and the model was fine-tuned with a lower learning rate. This strategy helps to adapt the pre-trained general features to the specific patterns present in the custom dataset, typically leading to improved accuracy. The printed validation accuracy reflects the model's performance after this fine-tuning step. The training and validation accuracy over epochs would show if the model is overfitting or underfitting (e.g., if training accuracy is much higher than validation accuracy, it indicates overfitting).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5d_ebpMllWW"
      },
      "source": [
        "8. Write a program to visualize the filters and feature maps of the first\n",
        "convolutional layer of AlexNet on an example input image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3djcvQedlgQB"
      },
      "outputs": [],
      "source": [
        "# Visualize filters and feature maps of AlexNet's first conv layer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications import imagenet_utils\n",
        "\n",
        "# 1. Define a simplified AlexNet-like model (first few layers)\n",
        "def alexnet_first_layer(input_shape=(227,227,3)):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(96, (11,11), strides=4, activation='relu', input_shape=input_shape, name='conv1'),\n",
        "        layers.MaxPooling2D(pool_size=(3,3), strides=2)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 2. Load an example image\n",
        "img_path = 'path_to_example_image.jpg'  # Replace with your image path\n",
        "img = image.load_img(img_path, target_size=(227,227))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = tf.keras.applications.alexnet.preprocess_input(img_array) if hasattr(tf.keras.applications,'alexnet') else img_array/255.0\n",
        "\n",
        "# 3. Create model and get the first conv layer\n",
        "model = alexnet_first_layer()\n",
        "first_conv_layer = model.get_layer('conv1')\n",
        "\n",
        "# 4. Visualize filters of the first conv layer\n",
        "filters, biases = first_conv_layer.get_weights()\n",
        "print(f\"Filter shape: {filters.shape}\")\n",
        "\n",
        "n_filters = min(filters.shape[-1], 6)  # visualize first 6 filters\n",
        "fig, axes = plt.subplots(1, n_filters, figsize=(20,5))\n",
        "for i in range(n_filters):\n",
        "    f = filters[:,:,:,i]\n",
        "    f_min, f_max = f.min(), f.max()\n",
        "    f = (f - f_min) / (f_max - f_min)  # normalize\n",
        "    axes[i].imshow(f[:,:,0], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.suptitle('First Convolutional Layer Filters')\n",
        "plt.show()\n",
        "\n",
        "# 5. Compute feature maps for the input image\n",
        "feature_map_model = models.Model(inputs=model.input, outputs=first_conv_layer.output)\n",
        "feature_maps = feature_map_model.predict(img_array)\n",
        "\n",
        "# 6. Visualize the feature maps\n",
        "n_features = min(feature_maps.shape[-1], 6)  # visualize first 6 feature maps\n",
        "fig, axes = plt.subplots(1, n_features, figsize=(20,5))\n",
        "for i in range(n_features):\n",
        "    axes[i].imshow(feature_maps[0,:,:,i], cmap='viridis')\n",
        "    axes[i].axis('off')\n",
        "plt.suptitle('Feature Maps of First Conv Layer')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VH6SREzGlrlj"
      },
      "source": [
        "9. Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n",
        "overfitting or underfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX__P8WIloj6"
      },
      "outputs": [],
      "source": [
        "# Training InceptionV3 (GoogLeNet variant) on CIFAR-10\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# 1. Load and preprocess CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Resize images to match InceptionV3 input (299x299)\n",
        "x_train = tf.image.resize(x_train, (299,299))\n",
        "x_test = tf.image.resize(x_test, (299,299))\n",
        "\n",
        "# Preprocess inputs for InceptionV3\n",
        "x_train = preprocess_input(x_train)\n",
        "x_test = preprocess_input(x_test)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Load pre-trained InceptionV3 (without top layers)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Add custom top layers for CIFAR-10\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Train the model and record time\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "# 6. Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "# 7. Plot training and validation accuracy\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 8. Analyze Overfitting/Underfitting\n",
        "\"\"\"\n",
        "- If training accuracy is high but validation accuracy lags behind, it indicates overfitting.\n",
        "- If both training and validation accuracy are low, it indicates underfitting.\n",
        "- Using dropout, data augmentation, or fine-tuning the base model can help improve generalization.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBXSXMz_l1bE"
      },
      "source": [
        "10. You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "\n",
        "\n",
        "ans:\n",
        "\n",
        "\n",
        "# Approach for Limited Data in Medical X-ray Classification\n",
        "\n",
        "### Approach for Limited Data in Medical X-ray Classification\n",
        "\n",
        "1. Recommended Approach: Transfer Learning with Pre-trained CNNs\n",
        "   - Given the limited labeled dataset, the most effective approach is to use transfer learning\n",
        "     with a pre-trained CNN such as ResNet50, ResNet101, or InceptionV3.\n",
        "\n",
        "2. Justification:\n",
        "   - Feature Reuse: Pre-trained models have learned rich hierarchical features (edges, textures, shapes)\n",
        "     from large datasets like ImageNet, which are transferable to medical imaging tasks.\n",
        "   - Reduced Data Requirement: Training a CNN from scratch on limited X-ray data can easily lead to overfitting.\n",
        "     Fine-tuning a pre-trained model mitigates this.\n",
        "   - State-of-the-Art Performance: ResNet and Inception variants perform strongly in medical imaging\n",
        "     benchmarks, including pneumonia and COVID-19 detection.\n",
        "   - Flexibility: Freeze lower layers (generic features) and fine-tune top layers (task-specific),\n",
        "     balancing performance and overfitting risk.\n",
        "\n",
        "3. Suggested Pipeline:\n",
        "   - Data Preparation:\n",
        "     * Organize images into normal, pneumonia, and COVID-19 folders.\n",
        "     * Apply data augmentation (rotation, scaling, horizontal flip) to increase dataset size and improve generalization.\n",
        "   - Model Selection & Training:\n",
        "     * Load a pre-trained model (ResNet50 or InceptionV3) without the top classification layer.\n",
        "     * Add custom top layers: GlobalAveragePooling2D -> Dense(128, ReLU) -> Dropout(0.5) -> Dense(3, softmax)\n",
        "     * Freeze base layers initially, train only top layers.\n",
        "     * Optionally fine-tune the last convolutional block.\n",
        "     * Use categorical crossentropy loss and Adam optimizer.\n",
        "   - Evaluation:\n",
        "     * Use metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
        "     * Employ cross-validation to ensure robust performance with limited data.\n",
        "\n",
        "### Deployment Strategy for Production\n",
        "\n",
        "1. Model Export:\n",
        "   - Save the trained model in a portable format (e.g., TensorFlow SavedModel, ONNX)\n",
        "\n",
        "2. Inference API:\n",
        "   - Wrap the model in a REST API using FastAPI or Flask.\n",
        "   - Accept X-ray images as input and return predictions (normal/pneumonia/COVID-19) with confidence scores.\n",
        "\n",
        "3. Scalability & Monitoring:\n",
        "   - Deploy using Docker containers for reproducibility.\n",
        "   - Use cloud platforms (AWS, GCP, Azure) for scalability.\n",
        "   - Implement logging and monitoring to track prediction performance and detect model drift.\n",
        "\n",
        "4. User Interface:\n",
        "   - Develop a simple web/desktop interface for radiologists to upload X-ray images and view predictions.\n",
        "   - Include visualization of activation maps or Grad-CAM to highlight regions influencing model decisions.\n",
        "\n",
        "5. Compliance & Security:\n",
        "   - Ensure compliance with HIPAA or local healthcare regulations.\n",
        "   - Encrypt patient data in transit and at rest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2f0izk1mW7K"
      },
      "outputs": [],
      "source": [
        "# Transfer Learning for X-ray Classification: Normal, Pneumonia, COVID-19\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Load Custom Dataset\n",
        "# -----------------------------\n",
        "# Dataset folder structure:\n",
        "# dataset/train/normal, dataset/train/pneumonia, dataset/train/covid\n",
        "# dataset/validation/normal, etc.\n",
        "\n",
        "train_dir = \"dataset/train\"\n",
        "val_dir = \"dataset/validation\"\n",
        "\n",
        "img_size = (224, 224)  # ResNet50 input size\n",
        "\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=img_size,\n",
        "    batch_size=32,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "val_ds = image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=img_size,\n",
        "    batch_size=32,\n",
        "    label_mode='categorical'\n",
        ")\n",
        "\n",
        "num_classes = len(train_ds.class_names)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Load Pre-trained ResNet50\n",
        "# -----------------------------\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Build Model with Custom Top Layers\n",
        "# -----------------------------\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Train the Model\n",
        "# -----------------------------\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Evaluate Model\n",
        "# -----------------------------\n",
        "val_loss, val_accuracy = model.evaluate(val_ds)\n",
        "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Plot Training and Validation Accuracy\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Deployment Notes\n",
        "# -----------------------------\n",
        "\"\"\"\n",
        "Deployment Strategy:\n",
        "1. Save the trained model:\n",
        "   model.save(\"xray_resnet_model\")\n",
        "\n",
        "2. Wrap model in a REST API using FastAPI or Flask:\n",
        "   - Accept X-ray images via POST requests.\n",
        "   - Preprocess images, call model.predict(), return class & confidence.\n",
        "\n",
        "3. Containerize using Docker for reproducibility and scalability.\n",
        "\n",
        "4. Cloud deployment on AWS/GCP/Azure with GPU support.\n",
        "\n",
        "5. Optional: Use Grad-CAM for explainability:\n",
        "   - Visualize regions in the X-ray that influenced predictions.\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}